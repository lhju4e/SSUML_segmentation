{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation with tf.keras - 변형\n",
    "\n",
    "*img size증가, u-net layer 증가 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (3.0.2)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2.7.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2.18.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (1.22)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (4.32.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2018.8.24)\n",
      "Requirement already satisfied: text-unidecode==1.2 in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from python-slugify->kaggle) (1.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\hansung\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages (from requests->kaggle) (2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 다운/설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Upload the API token.\n",
    "def get_kaggle_credentials():\n",
    "  token_dir = os.path.join(os.path.expanduser(\"~\"),\".kaggle\")\n",
    "  token_file = os.path.join(token_dir, \"kaggle.json\")\n",
    "  if not os.path.isdir(token_dir):\n",
    "    os.mkdir(token_dir)\n",
    "  try:\n",
    "    with open(token_file,'r') as f:\n",
    "      pass\n",
    "  except IOError as no_file:\n",
    "    try:\n",
    "      from google.colab import files\n",
    "    except ImportError:\n",
    "      raise no_file\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if \"kaggle.json\" not in uploaded:\n",
    "      raise ValueError(\"You need an API key! see: \"\n",
    "                       \"https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
    "    with open(token_file, \"wb\") as f:\n",
    "      f.write(uploaded[\"kaggle.json\"])\n",
    "    os.chmod(token_file, 600)\n",
    "\n",
    "get_kaggle_credentials()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_name = 'carvana-image-masking-challenge'\n",
    "\n",
    "def load_data_from_zip(competition, file):\n",
    "  with zipfile.ZipFile(os.path.join(competition, file), \"r\") as zip_ref:\n",
    "    unzipped_file = zip_ref.namelist()[0]\n",
    "    zip_ref.extractall(competition)\n",
    "\n",
    "def get_data(competition):\n",
    "    kaggle.api.competition_download_files(competition, competition)\n",
    "    load_data_from_zip(competition, 'train.zip')\n",
    "    load_data_from_zip(competition, 'train_masks.zip')\n",
    "    load_data_from_zip(competition, 'train_masks.csv.zip')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(competition_name)\n",
    "img_dir = os.path.join(competition_name, \"train\")\n",
    "label_dir = os.path.join(competition_name, \"train_masks\")\n",
    "df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "x_train_filenames = []\n",
    "y_train_filenames = []\n",
    "for img_id in ids_train:\n",
    "  x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
    "  y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))\n",
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)    \n",
    "num_train_examples = len(x_train_filenames)\n",
    "num_val_examples = len(x_val_filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (512, 512, 3)\n",
    "batch_size = 5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_jpeg(img_str, channels=3)\n",
    "\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  # These are gif images so they return as (num_frames, h, w, c)\n",
    "  label_img = tf.image.decode_gif(label_img_str)[0]\n",
    "  # The label image should only have values of 1 or 0, indicating pixel wise\n",
    "  # object (car) or not (background). We take the first channel only. \n",
    "  print(\"label_img shape : \" ,label_img.shape())\n",
    "  label_img = label_img[:, :, 0]\n",
    "  print(\"label_img shape : \" ,label_img.shape())\n",
    "  label_img = tf.expand_dims(label_img, axis=-1)\n",
    "  print(\"label_img shape : \" ,label_img.shape())\n",
    "  return img, label_img\n",
    "\n",
    "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
    "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
    "  if width_shift_range or height_shift_range:\n",
    "      if width_shift_range:\n",
    "        width_shift_range = tf.random_uniform([], \n",
    "                                              -width_shift_range * img_shape[1],\n",
    "                                              width_shift_range * img_shape[1])\n",
    "      if height_shift_range:\n",
    "        height_shift_range = tf.random_uniform([],\n",
    "                                               -height_shift_range * img_shape[0],\n",
    "                                               height_shift_range * img_shape[0])\n",
    "      # Translate both \n",
    "      output_img = tfcontrib.image.translate(output_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "      label_img = tfcontrib.image.translate(label_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "  return output_img, label_img\n",
    "\n",
    "def flip_img(horizontal_flip, tr_img, label_img):\n",
    "  if horizontal_flip:\n",
    "    flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "    tr_img, label_img = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                lambda: (tf.image.flip_left_right(tr_img), tf.image.flip_left_right(label_img)),\n",
    "                                lambda: (tr_img, label_img))\n",
    "  return tr_img, label_img\n",
    "\n",
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
    "             horizontal_flip=False,  # Random left right flip,\n",
    "             width_shift_range=0,  # Randomly translate the image horizontally\n",
    "             height_shift_range=0):  # Randomly translate the image vertically \n",
    "  if resize is not None:\n",
    "    # Resize both images\n",
    "    label_img = tf.image.resize_images(label_img, resize)\n",
    "    img = tf.image.resize_images(img, resize)\n",
    "  \n",
    "  if hue_delta:\n",
    "    img = tf.image.random_hue(img, hue_delta)\n",
    "  \n",
    "  img, label_img = flip_img(horizontal_flip, img, label_img)\n",
    "  img, label_img = shift_img(img, label_img, width_shift_range, height_shift_range)\n",
    "  label_img = tf.to_float(label_img) * scale\n",
    "  img = tf.to_float(img) * scale \n",
    "  return img, label_img\n",
    "\n",
    "def get_baseline_dataset(filenames, \n",
    "                         labels,\n",
    "                         preproc_fn=functools.partial(_augment),\n",
    "                         threads=5, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True):           \n",
    "  num_x = len(filenames)\n",
    "  # Create a dataset from the filenames and labels\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "  # Map our preprocessing function to every element in our dataset, taking\n",
    "  # advantage of multithreading\n",
    "  dataset = dataset.map(_process_pathnames, num_parallel_calls=threads)\n",
    "  if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
    "    assert batch_size == 1, \"Batching images must be of the same size\"\n",
    "\n",
    "  dataset = dataset.map(preproc_fn, num_parallel_calls=threads)\n",
    "  \n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(num_x)\n",
    "  \n",
    "  \n",
    "  # It's necessary to repeat our data for all epochs \n",
    "  dataset = dataset.repeat().batch(batch_size)\n",
    "  return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "    'hue_delta': 0.1,\n",
    "    'horizontal_flip': True,\n",
    "    'width_shift_range': 0.1,\n",
    "    'height_shift_range': 0.1\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)\n",
    "\n",
    "val_cfg = {\n",
    "    'resize': [img_shape[0], img_shape[1]],\n",
    "    'scale': 1 / 255.,\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)\n",
    "\n",
    "train_ds = get_baseline_dataset(x_train_filenames,\n",
    "                                y_train_filenames,\n",
    "                                preproc_fn=tr_preprocessing_fn,\n",
    "                                batch_size=batch_size)\n",
    "val_ds = get_baseline_dataset(x_val_filenames,\n",
    "                              y_val_filenames, \n",
    "                              preproc_fn=val_preprocessing_fn,\n",
    "                              batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=img_shape)\n",
    "# 512\n",
    "\n",
    "encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "# 256\n",
    "\n",
    "encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "# 128\n",
    "\n",
    "encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "# 64\n",
    "\n",
    "encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "# 32\n",
    "\n",
    "encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "# 16\n",
    "\n",
    "encoder5_pool, encoder5 = encoder_block(encoder4_pool, 1024)\n",
    "# 8\n",
    "\n",
    "center = conv_block(encoder5_pool, 2048)\n",
    "# center\n",
    "\n",
    "decoder5 = decoder_block(center, encoder5, 1024)\n",
    "# 16\n",
    "\n",
    "decoder4 = decoder_block(decoder5, encoder4, 512)\n",
    "# 32\n",
    "\n",
    "decoder3 = decoder_block(decoder4, encoder3, 256)\n",
    "# 64\n",
    "\n",
    "decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "# 128\n",
    "\n",
    "decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "# 256\n",
    "\n",
    "decoder0 = decoder_block(decoder1, encoder0, 32)\n",
    "# 512\n",
    "\n",
    "outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능평가랑 loss 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/tmp/weights.hdf5'\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, \n",
    "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "                   epochs=epochs,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "                   callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = history.history['dice_loss']\n",
    "val_dice = history.history['val_dice_loss']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, dice, label='Training Dice Loss')\n",
    "plt.plot(epochs_range, val_dice, label='Validation Dice Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Dice Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
