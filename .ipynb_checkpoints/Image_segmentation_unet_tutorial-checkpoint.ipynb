{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 튜토리얼 공부 - \n",
    "# image segmentation with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://simhyejin.github.io/2016/06/30/Markdown-syntax/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다룰 개념 : \n",
    "\n",
    "- funcional API : `UNet`을 구현할 것임. UNet은 CNN 모델이고, 보통 funtional API와 의학적 이미지 segmentation에 쓰인다.\n",
    "\n",
    " - 이 모델은 여러개의 input/output이 요구되는 레이어를 가진다. 이것은(이 모델은) functional API의 사용을 요구한다.\n",
    " - U-Net의 원본 [paper](https://arxiv.org/abs/1505.04597)를 확인해라.\n",
    " \n",
    "- Custom Loss Function and Metrics : `binary cross entropy` 와 `dice loss`를 이용하여 custom loss function을 구현한다. 또한 dice coefficient와 mean intersection over union을 구현할 것이다. 그것은 우리의 훈련 과정을 보는 것과, 어떻게 우리가 수행되고 있는 것을 판단할 지를 도와준다.\n",
    "\n",
    "- Saving and loading keras models : 우리의 가장 좋은 모델을 디스크에 저장할 것이다. 우리가 모델을 추론하거나 평가하고 싶을 때, 디스크에서 모델을 꺼낼 것이다.\n",
    "\n",
    "### 우리는 이 일반적인 workflow를 따를 것이다: \n",
    "\n",
    "1. 데이터를 시각화하고, `탐색적 데이터 분석`을 시행한다.\n",
    "2. 데이터 파이프라인을 구축하고 전처리를 한다.\n",
    "3. 모델을 생성한다.\n",
    "4. 모델을 훈련시킨다.\n",
    "5. 모델을 발전시킨다(최적화한다)\n",
    "6. 반복\n",
    "\n",
    "*공지* : 이 포스트는 머신러닝을 어느정도 아는 사람의 수준에 맞춰져 있고, 돌려보려면 GPU사용이 굉장히 권유됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**U-Net**\n",
    "\n",
    "뒤에서 설명\n",
    "\n",
    "**binary cross entropy?**\n",
    "\n",
    "두 개의 class 중 하나를 예측하는 task에 대한 cross entropy의 경우이다.\n",
    "\n",
    "**dice loss?**\n",
    "\n",
    "통계적으로 두 개의 유사도를 측정하기 위해서 사용함. \n",
    "\n",
    "IOU(intersection over union)과 유사하게, 예측 영역과 실제 영역이 얼마나 겹치느냐를 측정한다. 1에 가까울수록 좋고, 0에 가까울수록 좋지 않은 것\n",
    "\n",
    "**탐색적 데이터 분석(EDA-Exploratory data analysis)?**\n",
    "\n",
    "데이터를 분석하기 전에, 시각화 등으로 자료를 직관적으로 바라보는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!pip install kaggle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "#그래프 격자, 크기 설정\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfcontrib\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import backend as K  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the files\n",
    "\n",
    "우리는 Kaggle의 데이터셋을 다룰 것이기 때문에, 너의 캐글 계정에서 [creating an API Token](https://github.com/Kaggle/kaggle-api#api-credentials) 이 요구되고, 받은 것을 업로드해라\n",
    "\n",
    "데이터 다운로드 전에 [accept the competition rules](https://www.kaggle.com/c/carvana-image-masking-challenge/rules)  반드시 동의를 해줘야 한다.\n",
    "\n",
    "*api token업로드 및 데이터 다운로드 코드 생략*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df_train = pd.read_csv(os.path.join(competition_name, 'train_masks.csv'))\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "x_train_filenames = []\n",
    "y_train_filenames = []\n",
    "\n",
    "for img_id in ids_train:\n",
    "  x_train_filenames.append(os.path.join(img_dir, \"{}.jpg\".format(img_id)))\n",
    "  y_train_filenames.append(os.path.join(label_dir, \"{}_mask.gif\".format(img_id)))\n",
    "    \n",
    "x_train_filenames, x_val_filenames, y_train_filenames, y_val_filenames = \\\n",
    "                    train_test_split(x_train_filenames, y_train_filenames, test_size=0.2, random_state=42)  \n",
    "                   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "```\n",
    "데이터 읽어옴\n",
    "\n",
    "train에서 이미지 이름만 떼서(.jpg버림) ids_train에 넣어줌\n",
    "\n",
    "x_train은 학습데이터 y_train은 그거에 대한 정답(마스크 데이터)\n",
    " \n",
    "train데이터(불러온 전체 데이터)를 train과 val데이터로 나눔(8:2로)\n",
    "\n",
    "=> Number of training examples: 4070\n",
    "   \n",
    "   Number of validation examples: 1018\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "랜덤으로 5개 샘플 이미지로 출력해주는 것\n",
    "\n",
    "```\n",
    "display_num = 5\n",
    "\n",
    "r_choices = np.random.choice(num_train_examples, display_num)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i in range(0, display_num * 2, 2):\n",
    "  img_num = r_choices[i // 2]\n",
    "  x_pathname = x_train_filenames[img_num]\n",
    "  y_pathname = y_train_filenames[img_num]\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 1)\n",
    "  plt.imshow(mpimg.imread(x_pathname))\n",
    "  plt.title(\"Original Image\")\n",
    "  \n",
    "  example_labels = Image.open(y_pathname)\n",
    "  label_vals = np.unique(example_labels)\n",
    "  \n",
    "  plt.subplot(display_num, 2, i + 2)\n",
    "  plt.imshow(example_labels)\n",
    "  plt.title(\"Masked Image\")  \n",
    "  \n",
    "plt.suptitle(\"Examples of Images and their Masks\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "이제 파라미터 세팅을 시작하자. 이미지 shape를 표준화하고 resize할것이다.\n",
    "\n",
    "```\n",
    "img_shape = (256, 256, 3)\n",
    "batch_size = 3\n",
    "epochs = 5\n",
    "```\n",
    "\n",
    "위와 같은 마라미터로 하면 너의 하드웨어가 계산하는데 힘들 것이다. 그러니까 수정해라. 우리의 UNet version의 구조를 아는 것이 중요하다, 이미지의 사이즈는 반드시 32의 인수인 짝수로 나눠지게 해야한다. 공간 resolution downsample을 MaxPooling2Dlayer를 사용해서 2의 인자가 되도록 하기 때문이다.\n",
    "\n",
    "만약에 너의 컴퓨터가 사양이 된다면, 너는 더 높은 input image resolution 으로 더 나은 성능이 되도록 할 수 있다. 그것은 더욱 정확한 localization이랑 encoding시에 더 적은 정보손실을 가능하게 한다. 추가로, 모델을 더 깊게 만들 수 있다. (그냥 하드웨어가 좋으면 성능 올릴 수 있으니까 올려보라는 말)\n",
    "\n",
    "대안으로, 너의 컴퓨터가 뒷바침이 안되면, image resolution 과(또는) batch size를 줄여라. image resolution을 낮추면 성능이 떨어지고, batch size를 줄이면 training 시간이 올라갈 것이라는 것을 염두해 둬라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our input pipeline with tf.data\n",
    "\n",
    "우리가 filename으로 시작했기 대문에, 우리는 우리의 모델에서 잘 돌아갈, 유연하고 확장 가능한 data pipeline를 만들어야 한다. 만약에 너가 tf.data랑 친숙하지 않으면, 그 개념에 대해 소개하는 내 다른 튜토리얼을 봐라\n",
    "\n",
    "#### our input pipeline will consist of the following steps:\n",
    "\n",
    "1. filename으로부터 파일의 byte를 읽어온다 - 이미지랑 레이블에서 동시에. 우리의 label은 실제로는 이미지에 차냐 배경이냐를 1, 0로 나타낸 것임을 상기해라.\n",
    "2. byte를 이미지 형식으로 decode해라\n",
    "3. 이미지 변형을 적용해라 (선택적으로, input 파라미터에 따라서)\n",
    " - `resize` : 우리의 이미지를 표준 size로 resize해라 (EDA(위에있음)나 메모리 제한에 의해 결정된 것으로)\n",
    "    - 이것이 선택사항인 이유는 U-Net이 fully convolutional network이라(FC layer가 없음) 인풋 사이즈에 의존적이지 않기 때문이다. 그러나, 만약에 너가 이미지 resize하지 않는 것을 선택하면, 너는 반드시 batch size를 1로 해야한다. 왜나하면 batch를 가능한 이미지 사이즈랑 같이 할 수 없어서 \n",
    "> 이 윗줄 무슨말인지 이해 안감.\n",
    "    - 대안으로, 너는 각 mini-batch마다 너의 이미지를 같이 묶고 resize할 수 있다. 이미지 resize를 너무 많이 하는 것을 피하기 위해. (너무 많이 하는 것은) 너의 성능에 간섭한다.(아마도)\n",
    " - `hue_delta` : RGB의 색을 랜덤 요인으로 조정해라. 이것은 우리의 실제 이미지에만 적용된다. `hue_delta`는 반드시 [0, 0.5]사이여야 한다.\n",
    " - `horizontal_flip` : 0.5 확률로 가운데 축을 따라서 수평으로 이미지를 flip해라. 이 변환은 label이랑 실제 이미지에 둘 다 적용된다.\n",
    " - `width_shift_range` and `height_shift_range` 는 수평이나 수직으로 임의로 변환한 이미지의 범위 안이다.\n",
    " - `rescale` : 특정한 요인으로 이미지를 rescale해준다. ex) 1/255\n",
    "4. 데이터를 섞고, 반복하고(그래서 우리는 그것을 여러 번 반복할 수 있음), 데이터를 batch하고, 그리고 batch를 선인출(먼저 뽑아서 돌려보는?? 효율을 위해)한다.\n",
    "\n",
    "데이터 파이프라인에서 발생하는 이러한 변환은 상징적인 변환이어야 한다는 점에 유의해라.\n",
    "\n",
    "#### 왜 우리는 이미지 transformation을 하나\n",
    "\n",
    "이것은 data augmentation으로 알려져 있다. data augmentation은 임의의 변형을 통해 데이터를 증식시켜서 training data를 \"늘린다\".(이미지를 뒤집거나 crop하거나 색을 변형하거나 그런것들) 학습 시간에, 우리의 모델은 같은 사진을 두 번 보지 않을 것이다. 이는 overfitting을 방지해주고 모델이 처음 보는 데이터에 보편적으로 적용되는 것을 도와준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing each pathname\n",
    "\n",
    "```\n",
    "def _process_pathnames(fname, label_path):\n",
    "  # We map this function onto each pathname pair  \n",
    "  img_str = tf.read_file(fname)\n",
    "  img = tf.image.decode_jpeg(img_str, channels=3)\n",
    "  label_img = tf.image.decode_gif(label_img_str)[0]\n",
    "  label_img_str = tf.read_file(label_path)\n",
    "  # These are gif images so they return as (num_frames, h, w, c)\n",
    "  \n",
    "  # The label image should only have values of 1 or 0, indicating pixel wise\n",
    "  # object (car) or not (background). We take the first channel only. \n",
    "  label_img = label_img[:, :, 0]\n",
    "  label_img = tf.expand_dims(label_img, axis=-1)\n",
    "  return img, label_img\n",
    "```\n",
    "-----------------------\n",
    "```\n",
    "이 함수를 각 pathname 짝에 mapping한다.\n",
    "label 이미지는 gif이미지이고, (num_frames, h, w, c)형식으로 리턴한다.\n",
    "픽셀값이 1 또는 0이므로, 첫 번재 채널만 사용한다.\n",
    "tf.expand_dims : 텐서 shape바꿔줌\n",
    "```\n",
    "\n",
    "> 텐서 shape좀 물어보기. 텐서 어려워 ㅡㅡ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting the image\n",
    "\n",
    "```\n",
    "def shift_img(output_img, label_img, width_shift_range, height_shift_range):\n",
    "  \"\"\"This fn will perform the horizontal or vertical shift\"\"\"\n",
    "  if width_shift_range or height_shift_range:\n",
    "      if width_shift_range:\n",
    "        width_shift_range = tf.random_uniform([], \n",
    "                                              -width_shift_range * img_shape[1],\n",
    "                                              width_shift_range * img_shape[1])\n",
    "      if height_shift_range:\n",
    "        height_shift_range = tf.random_uniform([],\n",
    "                                               -height_shift_range * img_shape[0],\n",
    "                                               height_shift_range * img_shape[0])\n",
    "      # Translate both \n",
    "      output_img = tfcontrib.image.translate(output_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "      label_img = tfcontrib.image.translate(label_img,\n",
    "                                             [width_shift_range, height_shift_range])\n",
    "  return output_img, label_img\n",
    "```\n",
    "----------\n",
    "```\n",
    "tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None) : minval ~ maxval 사이의 난수값 생성. shape로 정해진 텐서\n",
    "tf.contrib.image.translate(images, translations,interpolation='NEAREST',name=None) : 이미지를 옮기는 함수. 옮겨서 빈 공간은 0으로 채운다고 한다.\n",
    "-shift_range * img_shape ~ shift_range * img_shape 사이의 임의의 값으로 이미지를 옮ㄱㅕ주는 것 같다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping the image randomly\n",
    "\n",
    "*이미지 뒤집어주기 - 코드 생략*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling our transformations into our augment function\n",
    "\n",
    "```\n",
    "def _augment(img,\n",
    "             label_img,\n",
    "             resize=None,  # Resize the image to some size e.g. [256, 256]\n",
    "             scale=1,  # Scale image e.g. 1 / 255.\n",
    "             hue_delta=0,  # Adjust the hue of an RGB image by random factor\n",
    "             horizontal_flip=False,  # Random left right flip,\n",
    "             width_shift_range=0,  # Randomly translate the image horizontally\n",
    "             height_shift_range=0):  # Randomly translate the image vertically \n",
    "             \n",
    "def get_baseline_dataset(filenames, \n",
    "          labels,\n",
    "          preproc_fn=functools.partial(_augment),\n",
    "          threads=5, \n",
    "          batch_size=batch_size,\n",
    "          shuffle=True): \n",
    "```\n",
    "*함수 정의부랑 설명 생략*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and validation datasets\n",
    "\n",
    "image augmentation은 우리의 training 데이터 셋에만 적용하고 validation dataset에는 적용 안한다는 점 유의해라.\n",
    "\n",
    "*위에서 정의한 함수들 사용. 코드 생략*\n",
    "\n",
    "### Let's see if our image augmentor data pipeline is producing expected results\n",
    "\n",
    "*생략*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "우리는 U-Net 모델을 생성할 것이다. U-Net은 특히 segmentation에 뛰어난데, 높은 해상도의 마스크를 생성하기 위한 localize가 가능하기 때문이다. 추가로, 그것은 적은 데이터셋에서 잘 작동하고, 상대적으로 오버피팅에 견고하다. 왜냐하면 훈련 데이터는 이미지 내의 패치 수에 관한 것이고, 그것은 훈련 데이터 자체보다 크기 때문이다. (U-net 이미지 보면, patch size가 image size보다 큰데, 그거를 말하는 듯) 원본 이미지와 다르게, 우리의 블록 각각에 batch normalization을 추가할 것이다.\n",
    "\n",
    "Unet은 encoder 부분과 decoder 부분으로 이루어져 있다. (그림에서의 왼쪽 부분과 오른쪽 부분)\n",
    "encoder 부분은 Conv의 선형 스택, BatchNorm, Relu 연산과 뒤이은 MaxPool로 이루어져 있다. 각각의 MaxPool은 우리의 특성 맵의 공간 정보를 2의 인수로 축소시킨다. decoder 부분은 UpSampling2D, Conv, Batchnorm, Relu로 이루어져 있다. 우리는 decoder 부분에 같은 크기의 특성 맵을 연쇄시킬 수 있다. (그림에서의 회색 화살표) 마지막으로, 우리는 마지막 conv 연산을 추가해서, 우리의 마지막 segmentation mask ouput이 grayscale로 나오도록 한다.\n",
    "\n",
    "![U-Net](./image/u_net.png)\n",
    "\n",
    "\n",
    "추가 설명\n",
    "- encoder 부분에서는 2x2 max pooling과 stride 2를 사용한다.\n",
    "- downsampling시에, 2배의 feature channel을 사용한다.\n",
    "- padding 을 할 때, 일반적인 zero-padding이 아닌 mirror-padding을 사용한다. \n",
    "   - 밑에서 설명할 것.\n",
    "   - 맨 첫 번째 레이어를 보면, 572 x 572 => 570 x 570 이 되었다. 계산해보면 패딩을 284로 한 것으로 나오는데, 첫 번째 이미지 사이즈의 반이다. 이미지를 네 부분으로 쪼개서 각 부분들을 다 미러링 시킨 것을 패딩으로 썼다고 생각하면 된다.\n",
    "- decoder 부분에서는 feature channel을 반으로 줄여서 사용한다.\n",
    "- upsampling시에, 왼쪽의 encoder부분에서 Max pooling되기 전의 feature map을 가져와서 붙인다.\n",
    "- 마지막 final layer에서는 1x1 convolution을 사용하여 2개의 클래스로 분류 (배경이냐, 물체냐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mirroring](./image/mirroring.png)\n",
    "\n",
    "추가 설명\n",
    "- 그림의 파란색 부분이 Patch , 이미지 인식 단위이다.\n",
    "- 노란색 부분이 실제 segmentation 될 영역이다.\n",
    "- 한 번 인식한 patch 부분은 다음 부분에서 인식하지 않는다. (stride 2)\n",
    "- CNN에서 padding을 많이 깔지 않으면, 외곽 부분이 점점 깎여나간다. 이를 해결하기 위해 mirroring을 사용하였다.\n",
    "- Overlap-tile 전략이라고 한다. \n",
    "\n",
    "> 그것은 적은 데이터셋에서 잘 작동하고, 상대적으로 오버피팅에 견고하다. 왜냐하면 훈련 데이터는 이미지 내의 패치 수에 관한 것이고, 그것은 훈련 데이터 자체보다 크기 때문이다. (U-net 이미지 보면, patch size가 image size보다 큰데, 그거를 말하는 듯) 이게 왜 오버피팅에 견고한지? 모르겠네\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Keras Functional API\n",
    "\n",
    "Keras Functional API 는 너가 multi-input/output model, 공유 레이어 등을 사용할 때 쓴다. Keras Functional API는 너가 텐서를 조작하고, 데이터스트림이 뒤섞인 복합 그래프를 생성할 수 있도록 하는데에 효과적이다. 추가로, 그것은 tensor에서 layer와 model을 둘 다 부를 수 있도록 만든다.\n",
    "\n",
    "우리는 우리의 모델 block 연산들을 쉽고 간단하게 모을 수 있게 해주는 helper 함수들을 생성할 것이다.\n",
    "\n",
    "```\n",
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "  encoder = layers.BatchNormalization()(encoder)\n",
    "  encoder = layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = layers.BatchNormalization()(decoder)\n",
    "  decoder = layers.Activation('relu')(decoder)\n",
    "  return decoder\n",
    "  ```\n",
    "  -----------------------\n",
    "```\n",
    "\n",
    "conv_block은 그림에서 →→*\n",
    "\n",
    "encoder_block 은 그림에서 ↓\n",
    "\n",
    "decoder block 은 ↑→→\n",
    "\n",
    "decoder 할 때, upsampling, BN, 활성화 후 CNN 넣는거.. 순서..\n",
    "```\n",
    ">여기서는 그냥 ('same' 입출력 크기 유지)zero-padding 줬는데, 이래도 성능이..?\n",
    "\n",
    "*그 뒤의 블록 사용 코드는 생략*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your model\n",
    "\n",
    "functional API를 사용해서, 너는 반드시 모델과 관련된 입출력을 지정해서 모델을 정의해야 한다.\n",
    "\n",
    "```\n",
    "model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining custom metrics and loss functions\n",
    "\n",
    "loss랑 metric function들을 케라스를 이용해서 정의하는 것은 쉽다. 주어진 예제에 대한 True 라벨과 동일한 예에 대한 Predicted 라벨을 모두 사용하는 함수를 정의하기만 하면 된다.\n",
    "\n",
    "Dice loss는 겹침을 측정하는 metric(측정 척도)이다. 더 많은 정보는 [여기](http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf)에서 볼 수 있다.\n",
    "\n",
    "우리는 여기에 dice loss를 쓸 것이다. 왜냐하면 class 불균형 문제에서 더 잘 동작하기 때문이다. (의학데이터의 경우, 병이 맞는 클래스보다 아닌 클래스가 훨씬 더 많으니) 추가로, dice coefficient와  IoU metrics를 최대화하는 것은 실제 우리의 segmentation의 목표이다. cross entropy를 사용하는 것은 더 쉽게 최대화를 할 수 있는 대용품이다.? 대신에, 우리는 우리의 목표를 바로 최대화한다.\n",
    "\n",
    "*코드 생략*\n",
    "\n",
    "*텐서 평평하게 펴서 공식에 넣음. 이미지 겹치는 부분을 scoring하는 것*\n",
    "\n",
    "우리는, dice loss랑 binary cross entropy를 합치는 특별히(만든)함수를 이용할 것이다. 이것은 [individuals who competed within this competition obtaining better results empirically](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/40199)에 기반으로 되어있다. 너의 설정 loss를 성능 측정에 사용해봐라\n",
    "```\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "```\n",
    "*진짜.. 그냥 더하기만 했는데, 사실 쓸거면 앞에 가중치로 계수라도 붙여줘야 하는 것이 아닌가..?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile your model\n",
    "\n",
    "우리는 최소화시키기 위해 custom loss를 사용했다. 추가로, 우리는 훈련시킬 때 어떤 척도를 유지시키길 원하는지 특정화했다. metrics는 실제로 파라미터 튜닝 과정에 사용되지 않고, 대신에 training 과정 성능을 측정하는 데 사용된다는 것을 주의해라.\n",
    "\n",
    "## Train your model\n",
    "\n",
    "tf.data로 너의 모델을 훈련시키는 것은 너의 training/validation dataset, step, epoch의 수와 모델의 간단한 fit function을 제공하는 것을 포함한다.\n",
    "\n",
    "우리는 또한 model callback, ModelCheckpoint(함수)를 포함한다. 그것은 모델을 각 epoch 후에 디스크에 저장하게 해 줄 것이다. 우리는 가장 성능이 좋은 모델만 디스크에 저장하게 설정할 수 있다. 모델을 저장하는 것은 모델의 가중치 이상을 저장하는 것을 주의해라 : 기본으로, 모델 구조, 가중치, optimizer같은 훈련 과정에 대한 정보들도 포함되어 있다.\n",
    "\n",
    "```\n",
    "save_model_path = '/tmp/weights.hdf5'\n",
    "cp = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_path, monitor='val_dice_loss', save_best_only=True, verbose=1)\n",
    "```\n",
    "\n",
    "fit 함수를 호출해서 우리가 되돌아갈 지점을 정해놓는 것을 잊지마라.\n",
    "\n",
    "```\n",
    "history = model.fit(train_ds, \n",
    "                   steps_per_epoch=int(np.ceil(num_train_examples / float(batch_size))),\n",
    "                   epochs=epochs,\n",
    "                   validation_data=val_ds,\n",
    "                   validation_steps=int(np.ceil(num_val_examples / float(batch_size))),\n",
    "                   callbacks=[cp])\n",
    "```\n",
    "*이런식으로*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training process\n",
    "\n",
    "*시각화는 생략*\n",
    "\n",
    "> 함수 그래프 왜 중간에 튀나 궁금"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
